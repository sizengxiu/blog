<html>
<head>
  <title>Evernote Export</title>
  <basefont face="微软雅黑" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="YXBJ Windows/601748 (zh-CN, DDL); Windows/6.1.1 (Win64); EDAMVersion=V2;"/>
  <style>
    body, td {
      font-family: 微软雅黑;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="489"/>

<div>
<span><div>1、分片接口</div><div>    elastic-job有多种分片策略，他们直接必然存在某个共同点，盲猜是面向接口的，点开一个分片策略的实现类，果然发现了这个接口</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>/**</div><div>* 作业分片策略.</div><div>*</div><div>* @author zhangliang</div><div>*/</div><div>public interface JobShardingStrategy {</div><div>    </div><div>    /**</div><div>     * 作业分片.</div><div>     *</div><div>     * @param jobInstances 所有参与分片的单元列表</div><div>     * @param jobName 作业名称</div><div>     * @param shardingTotalCount 分片总数</div><div>     * @return 分片结果</div><div>     */</div><div>    Map&lt;JobInstance, List&lt;Integer&gt;&gt; sharding(List&lt;JobInstance&gt; jobInstances, String jobName, int shardingTotalCount);</div><div>}</div></div><div>分片策略只需要实现这个接口中的sharding方法即可。</div><div>查看继承树，发现elastic-job提供了三种分片策略，以下分别介绍</div><div><br/></div><div>2、平均分配</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>/**</div><div>* 基于平均分配算法的分片策略.</div><div>*</div><div>* &lt;p&gt;</div><div>* 如果分片不能整除, 则不能整除的多余分片将依次追加到序号小的服务器.</div><div>* 如:</div><div>* 1. 如果有3台服务器, 分成9片, 则每台服务器分到的分片是: 1=[0,1,2], 2=[3,4,5], 3=[6,7,8].</div><div>* 2. 如果有3台服务器, 分成8片, 则每台服务器分到的分片是: 1=[0,1,6], 2=[2,3,7], 3=[4,5].</div><div>* 3. 如果有3台服务器, 分成10片, 则每台服务器分到的分片是: 1=[0,1,2,9], 2=[3,4,5], 3=[6,7,8].</div><div>* &lt;/p&gt;</div><div>*</div><div>* @author zhangliang</div><div>*/</div><div>public final class AverageAllocationJobShardingStrategy implements JobShardingStrategy {</div><div>    </div><div>    @Override</div><div>    public Map&lt;JobInstance, List&lt;Integer&gt;&gt; sharding(final List&lt;JobInstance&gt; jobInstances, final String jobName, final int shardingTotalCount) {</div><div>        if (jobInstances.isEmpty()) {</div><div>            return Collections.emptyMap();</div><div>        }</div><div>        Map&lt;JobInstance, List&lt;Integer&gt;&gt; result = shardingAliquot(jobInstances, shardingTotalCount);</div><div>        addAliquant(jobInstances, shardingTotalCount, result);</div><div>        return result;</div><div>    }</div><div>    //计算每台服务器的平均分片数，多余的不管</div><div>    private Map&lt;JobInstance, List&lt;Integer&gt;&gt; shardingAliquot(final List&lt;JobInstance&gt; shardingUnits, final int shardingTotalCount) {</div><div>        Map&lt;JobInstance, List&lt;Integer&gt;&gt; result = new LinkedHashMap&lt;&gt;(shardingTotalCount, 1);</div><div>        int itemCountPerSharding = shardingTotalCount / shardingUnits.size();</div><div>        int count = 0;</div><div>        for (JobInstance each : shardingUnits) {</div><div>            List&lt;Integer&gt; shardingItems = new ArrayList&lt;&gt;(itemCountPerSharding + 1);</div><div>            for (int i = count * itemCountPerSharding; i &lt; (count + 1) * itemCountPerSharding; i++) {</div><div>                shardingItems.add(i);</div><div>            }</div><div>            result.put(each, shardingItems);</div><div>            count++;</div><div>        }</div><div>        return result;</div><div>    }</div><div>    //不够平均分配的分片，分在前几个服务器上</div><div>    private void addAliquant(final List&lt;JobInstance&gt; shardingUnits, final int shardingTotalCount, final Map&lt;JobInstance, List&lt;Integer&gt;&gt; shardingResults) {</div><div>        int aliquant = shardingTotalCount % shardingUnits.size();</div><div>        int count = 0;</div><div>        for (Map.Entry&lt;JobInstance, List&lt;Integer&gt;&gt; entry : shardingResults.entrySet()) {</div><div>            if (count &lt; aliquant) {</div><div>                entry.getValue().add(shardingTotalCount / shardingUnits.size() * shardingUnits.size() + count);</div><div>            }</div><div>            count++;</div><div>        }</div><div>    }</div><div>}</div></div><div>看别人这注释写的，一目了然。再看代码，简单明了。</div><div><br/></div><div>3、根据作业名的哈希值奇偶数决定IP升降序算法的分片策略.</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>/**</div><div>* 根据作业名的哈希值奇偶数决定IP升降序算法的分片策略.</div><div>*</div><div>* &lt;p&gt;</div><div>* 作业名的哈希值为奇数则IP升序.</div><div>* 作业名的哈希值为偶数则IP降序.</div><div>* 用于不同的作业平均分配负载至不同的服务器.</div><div>* 如:</div><div>* 1. 如果有3台服务器, 分成2片, 作业名称的哈希值为奇数, 则每台服务器分到的分片是: 1=[0], 2=[1], 3=[].</div><div>* 2. 如果有3台服务器, 分成2片, 作业名称的哈希值为偶数, 则每台服务器分到的分片是: 3=[0], 2=[1], 1=[].</div><div>* &lt;/p&gt;</div><div>*</div><div>* @author zhangliang</div><div>*/</div><div>public final class OdevitySortByNameJobShardingStrategy implements JobShardingStrategy {</div><div>    </div><div>    private AverageAllocationJobShardingStrategy averageAllocationJobShardingStrategy = new AverageAllocationJobShardingStrategy();</div><div>    </div><div>    @Override</div><div>    public Map&lt;JobInstance, List&lt;Integer&gt;&gt; sharding(final List&lt;JobInstance&gt; jobInstances, final String jobName, final int shardingTotalCount) {</div><div>        long jobNameHash = jobName.hashCode();</div><div>        if (0 == jobNameHash % 2) {</div><div>            Collections.reverse(jobInstances);</div><div>        }</div><div>        return averageAllocationJobShardingStrategy.sharding(jobInstances, jobName, shardingTotalCount);</div><div>    }</div><div>}</div></div><div>就是根据作业名哈希值的奇偶给服务器排了个序，然后再调用平均分配策略的代码，真会省事。</div><div><br/></div><div>4、根据作业名的哈希值对服务器列表进行轮转的分片策略.</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>/**</div><div>* 根据作业名的哈希值对服务器列表进行轮转的分片策略.</div><div>*</div><div>* @author weishubin</div><div>*/</div><div>public final class RotateServerByNameJobShardingStrategy implements JobShardingStrategy {</div><div>    </div><div>    private AverageAllocationJobShardingStrategy averageAllocationJobShardingStrategy = new AverageAllocationJobShardingStrategy();</div><div>    </div><div>    @Override</div><div>    public Map&lt;JobInstance, List&lt;Integer&gt;&gt; sharding(final List&lt;JobInstance&gt; jobInstances, final String jobName, final int shardingTotalCount) {</div><div>        return averageAllocationJobShardingStrategy.sharding(rotateServerList(jobInstances, jobName), jobName, shardingTotalCount);</div><div>    }</div><div>    </div><div>    private List&lt;JobInstance&gt; rotateServerList(final List&lt;JobInstance&gt; shardingUnits, final String jobName) {</div><div>        int shardingUnitsSize = shardingUnits.size();</div><div>        int offset = Math.abs(jobName.hashCode()) % shardingUnitsSize;</div><div>        if (0 == offset) {</div><div>            return shardingUnits;</div><div>        }</div><div>        List&lt;JobInstance&gt; result = new ArrayList&lt;&gt;(shardingUnitsSize);</div><div>        for (int i = 0; i &lt; shardingUnitsSize; i++) {</div><div>            int index = (i + offset) % shardingUnitsSize;</div><div>            result.add(shardingUnits.get(index));</div><div>        }</div><div>        return result;</div><div>    }</div><div>}</div></div><div>看了半天，这个发现貌似没啥用</div><div><br/></div><div>作业的执行是从JobRunShell.run方法开始的。</div><div>run方法执行了job.execute(jec);</div><div><br/></div><div>2、分片计算</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>/**</div><div>* Lite调度作业.</div><div>*</div><div>* @author zhangliang</div><div>*/</div><div>public final class LiteJob implements Job {</div><div>    </div><div>    @Setter</div><div>    private ElasticJob elasticJob;</div><div>    </div><div>    @Setter</div><div>    private JobFacade jobFacade;</div><div>    </div><div>    @Override</div><div>    public void execute(final JobExecutionContext context) throws JobExecutionException {</div><div>        JobExecutorFactory.getJobExecutor(elasticJob, jobFacade).execute();</div><div>    }</div><div>}</div></div><div>Job是quartz提供的接口</div><div>作业执行器工厂中就只有一个方法：根据作业类型获取获取作业执行器</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>/**</div><div>* 作业执行器工厂.</div><div>*</div><div>* @author zhangliang</div><div>*/</div><div>@NoArgsConstructor(access = AccessLevel.PRIVATE)</div><div>public final class JobExecutorFactory {</div><div>    </div><div>    /**</div><div>     * 获取作业执行器.</div><div>     *</div><div>     * @param elasticJob 分布式弹性作业</div><div>     * @param jobFacade 作业内部服务门面服务</div><div>     * @return 作业执行器</div><div>     */</div><div>    @SuppressWarnings(&quot;unchecked&quot;)</div><div>    public static AbstractElasticJobExecutor getJobExecutor(final ElasticJob elasticJob, final JobFacade jobFacade) {</div><div>        if (null == elasticJob) {</div><div>            return new ScriptJobExecutor(jobFacade);</div><div>        }</div><div>        if (elasticJob instanceof SimpleJob) {</div><div>            return new SimpleJobExecutor((SimpleJob) elasticJob, jobFacade);</div><div>        }</div><div>        if (elasticJob instanceof DataflowJob) {</div><div>            return new DataflowJobExecutor((DataflowJob) elasticJob, jobFacade);</div><div>        }</div><div>        throw new JobConfigurationException(&quot;Cannot support job type '%s'&quot;, elasticJob.getClass().getCanonicalName());</div><div>    }</div><div>}</div><div>方法中的是三个作业执行器都继承自AbstractElasticJobExecutor</div></div><div>execute()方法实际执行的是父类的方法</div><div><br/></div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>/**</div><div>* 执行作业.</div><div>*/</div><div>public final void execute() {</div><div>    try {</div><div>        jobFacade.checkJobExecutionEnvironment();</div><div>    } catch (final JobExecutionEnvironmentException cause) {</div><div>        jobExceptionHandler.handleException(jobName, cause);</div><div>    }</div><div>    ShardingContexts shardingContexts = jobFacade.getShardingContexts();</div><div>    if (shardingContexts.isAllowSendJobEvent()) {</div><div>        jobFacade.postJobStatusTraceEvent(shardingContexts.getTaskId(), State.TASK_STAGING, String.format(&quot;Job '%s' execute begin.&quot;, jobName));</div><div>    }</div><div>    if (jobFacade.misfireIfRunning(shardingContexts.getShardingItemParameters().keySet())) {</div><div>        if (shardingContexts.isAllowSendJobEvent()) {</div><div>            jobFacade.postJobStatusTraceEvent(shardingContexts.getTaskId(), State.TASK_FINISHED, String.format(</div><div>                    &quot;Previous job '%s' - shardingItems '%s' is still running, misfired job will start after previous job completed.&quot;, jobName,</div><div>                    shardingContexts.getShardingItemParameters().keySet()));</div><div>        }</div><div>        return;</div><div>    }</div><div>    jobFacade.cleanPreviousExecutionInfo();</div><div>    try {</div><div>        jobFacade.beforeJobExecuted(shardingContexts);</div><div>        //CHECKSTYLE:OFF</div><div>    } catch (final Throwable cause) {</div><div>        //CHECKSTYLE:ON</div><div>        jobExceptionHandler.handleException(jobName, cause);</div><div>    }</div><div>    execute(shardingContexts, JobExecutionEvent.ExecutionSource.NORMAL_TRIGGER);</div><div>    while (jobFacade.isExecuteMisfired(shardingContexts.getShardingItemParameters().keySet())) {</div><div>        jobFacade.clearMisfire(shardingContexts.getShardingItemParameters().keySet());</div><div>        execute(shardingContexts, JobExecutionEvent.ExecutionSource.MISFIRE);</div><div>    }</div><div>    jobFacade.failoverIfNecessary();</div><div>    try {</div><div>        jobFacade.afterJobExecuted(shardingContexts);</div><div>        //CHECKSTYLE:OFF</div><div>    } catch (final Throwable cause) {</div><div>        //CHECKSTYLE:ON</div><div>        jobExceptionHandler.handleException(jobName, cause);</div><div>    }</div><div>}</div></div><div>实际执行分片计算的是在JobFacade.getShardingContexts()</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>public ShardingContexts getShardingContexts() {</div><div>    boolean isFailover = configService.load(true).isFailover();</div><div>    if (isFailover) {</div><div>        List&lt;Integer&gt; failoverShardingItems = failoverService.getLocalFailoverItems();</div><div>        if (!failoverShardingItems.isEmpty()) {</div><div>            return executionContextService.getJobShardingContext(failoverShardingItems);</div><div>        }</div><div>    }</div><div>    //分片计算</div><div>    shardingService.shardingIfNecessary();</div><div>    List&lt;Integer&gt; shardingItems = shardingService.getLocalShardingItems();</div><div>    if (isFailover) {</div><div>        shardingItems.removeAll(failoverService.getLocalHostTakeOffItems());</div><div>    }</div><div>    shardingItems.removeAll(executionService.getDisabledItems(shardingItems));</div><div>    return executionContextService.getJobShardingContext(shardingItems);</div><div>}</div></div><div>分片计算</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>/**</div><div>* 如果需要分片且当前节点为主节点, 则作业分片.</div><div>*</div><div>* &lt;p&gt;</div><div>* 如果当前无可用节点则不分片.</div><div>* &lt;/p&gt;</div><div>*/</div><div>public void shardingIfNecessary() {</div><div>    //从zookeeper的instances结点下读取运行实例列表</div><div>    List&lt;JobInstance&gt; availableJobInstances = instanceService.getAvailableJobInstances();</div><div>    //不需要分片或者实例列表为空则直接返回</div><div>    //此处isNeedSharding查看zookeepr上leader/sharding/necessary结点是否存在来判断是否需要分片</div><div>    if (!isNeedSharding() || availableJobInstances.isEmpty()) {</div><div>        return;</div><div>    }</div><div>    //判断是否已经有主节点，没有则发起一轮选举，最终都返回当前服务是不是leader</div><div>    if (!leaderService.isLeaderUntilBlock()) {</div><div>        //如果不是leader,就循环阻塞，直至分片结束</div><div>        blockUntilShardingCompleted();</div><div>        return;</div><div>    }</div><div>    //等上一轮作业执行完毕</div><div>    waitingOtherJobCompleted();</div><div>    //从缓存中加载作业信息</div><div>    LiteJobConfiguration liteJobConfig = configService.load(false);</div><div>    int shardingTotalCount = liteJobConfig.getTypeConfig().getCoreConfig().getShardingTotalCount();</div><div>    log.debug(&quot;Job '{}' sharding begin.&quot;, jobName);</div><div>    在zookeeper中创建临时节点：leader/sharding/processing</div><div>    jobNodeStorage.fillEphemeralJobNode(ShardingNode.PROCESSING, &quot;&quot;);</div><div>    resetShardingInfo(shardingTotalCount);</div><div>    JobShardingStrategy jobShardingStrategy = JobShardingStrategyFactory.getStrategy(liteJobConfig.getJobShardingStrategyClass());</div><div>    //开始执行分片操作，其中sharding是计算分片，callback是在zookeeper记录分片信息并删除zookeeper需要分片与正在分片标记，所以服务第一次启动后分片，后续如果没有作业变更或者服务实例变更不用再重新计算分片</div><div>    jobNodeStorage.executeInTransaction(new PersistShardingInfoTransactionExecutionCallback(jobShardingStrategy.sharding(availableJobInstances, jobName, shardingTotalCount)));</div><div>    log.debug(&quot;Job '{}' sharding complete.&quot;, jobName);</div><div>}</div></div><div>isLeaderUntilBlock()</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>/**</div><div>* 判断当前节点是否是主节点.</div><div>*</div><div>* &lt;p&gt;</div><div>* 如果主节点正在选举中而导致取不到主节点, 则阻塞至主节点选举完成再返回.</div><div>* &lt;/p&gt;</div><div>*</div><div>* @return 当前节点是否是主节点</div><div>*/</div><div>public boolean isLeaderUntilBlock() {</div><div>    //hasLeader查看zookeeper中leader/election/instance结点存不存在</div><div>    //读取zookeeper中instances结点下细腻</div><div>    while (!hasLeader() &amp;&amp; serverService.hasAvailableServers()) {</div><div>        log.info(&quot;Leader is electing, waiting for {} ms&quot;, 100);</div><div>        BlockUtils.waitingShortTime();</div><div>        if (!JobRegistry.getInstance().isShutdown(jobName) &amp;&amp; serverService.isAvailableServer(JobRegistry.getInstance().getJobInstance(jobName).getIp())) {</div><div>            electLeader();//发起新一轮选举</div><div>        }</div><div>    }</div><div>    return isLeader();</div><div>}</div></div><div>如果当前节点是leader节点，开始执行分片前，先判断上一轮的作业有没有执行完毕，等上一轮的作业执行完成后才开始这一轮的分片计算。waitingOtherJobCompleted()</div><div><br/></div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>//判断有误作业在执行，循环等待</div><div>private void waitingOtherJobCompleted() {</div><div>    while (executionService.hasRunningItems()) {</div><div>        log.debug(&quot;Job '{}' sleep short time until other job completed.&quot;, jobName);</div><div>        BlockUtils.waitingShortTime();</div><div>    }</div><div>}</div><div>/**</div><div>* 判断是否还有执行中的作业.</div><div>*</div><div>* @return 是否还有执行中的作业</div><div>*/</div><div>public boolean hasRunningItems() {</div><div>    return hasRunningItems(getAllItems());</div><div>}</div><div>//返回所有分片</div><div>private List&lt;Integer&gt; getAllItems() {</div><div>    int shardingTotalCount = configService.load(true).getTypeConfig().getCoreConfig().getShardingTotalCount();</div><div>    List&lt;Integer&gt; result = new ArrayList&lt;&gt;(shardingTotalCount);</div><div>    for (int i = 0; i &lt; shardingTotalCount; i++) {</div><div>        result.add(i);</div><div>    }</div><div>    return result;</div><div>}</div><div>/**</div><div>* 判断分片项中是否还有执行中的作业.</div><div>*</div><div>* @param items 需要判断的分片项列表</div><div>* @return 分片项中是否还有执行中的作业</div><div>*/</div><div>public boolean hasRunningItems(final Collection&lt;Integer&gt; items) {</div><div>    //从缓存中取数据</div><div>    LiteJobConfiguration jobConfig = configService.load(true);</div><div>    if (null == jobConfig || !jobConfig.isMonitorExecution()) {</div><div>        return false;</div><div>    }</div><div>    for (int each : items) {</div><div>        //zookeeper中的sharding/running下结点</div><div>        if (jobNodeStorage.isJobNodeExisted(ShardingNode.getRunningNode(each))) {</div><div>            return true;</div><div>        }</div><div>    }</div><div>    return false;</div><div>}</div></div><div>重新分片前先重设zookeeper中的分片信息resetShardingInfo，主要是删掉多余的</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>private void resetShardingInfo(final int shardingTotalCount) {</div><div>    for (int i = 0; i &lt; shardingTotalCount; i++) {</div><div>        jobNodeStorage.removeJobNodeIfExisted(ShardingNode.getInstanceNode(i));//sharding/instance</div><div>        jobNodeStorage.createJobNodeIfNeeded(ShardingNode.ROOT + &quot;/&quot; + i);//sharding</div><div>    }</div><div>    //如果这次作业分片数量比上次少，就把多的删掉</div><div>    int actualShardingTotalCount = jobNodeStorage.getJobNodeChildrenKeys(ShardingNode.ROOT).size();</div><div>    if (actualShardingTotalCount &gt; shardingTotalCount) {</div><div>        for (int i = shardingTotalCount; i &lt; actualShardingTotalCount; i++) {</div><div>            jobNodeStorage.removeJobNodeIfExisted(ShardingNode.ROOT + &quot;/&quot; + i);</div><div>        }</div><div>    }</div><div>}</div></div><div>分片后的善后工作</div><div style="box-sizing: border-box; padding: 8px; font-family: Monaco, Menlo, Consolas, &quot;Courier New&quot;, monospace; font-size: 12px; color: rgb(51, 51, 51); border-radius: 4px; background-color: rgb(251, 250, 248); border: 1px solid rgba(0, 0, 0, 0.15);-en-codeblock:true;"><div>@RequiredArgsConstructor</div><div>class PersistShardingInfoTransactionExecutionCallback implements TransactionExecutionCallback {</div><div>    </div><div>    private final Map&lt;JobInstance, List&lt;Integer&gt;&gt; shardingResults;</div><div>    </div><div>    @Override</div><div>    public void execute(final CuratorTransactionFinal curatorTransactionFinal) throws Exception {</div><div>        for (Map.Entry&lt;JobInstance, List&lt;Integer&gt;&gt; entry : shardingResults.entrySet()) {</div><div>            for (int shardingItem : entry.getValue()) {</div><div>                //把分片信息记录在zookeeper的sharding/instance中</div><div>                curatorTransactionFinal.create().forPath(jobNodePath.getFullPath(ShardingNode.getInstanceNode(shardingItem)), entry.getKey().getJobInstanceId().getBytes()).and();</div><div>            }</div><div>        }</div><div>        //删除需要分片标记leader/sharding/necessary</div><div>        curatorTransactionFinal.delete().forPath(jobNodePath.getFullPath(ShardingNode.NECESSARY)).and();</div><div>        //删除正在分片中标记leader/sharding/processing</div><div>        curatorTransactionFinal.delete().forPath(jobNodePath.getFullPath(ShardingNode.PROCESSING)).and();</div><div>    }</div><div>}</div></div><div><br/></div><div>分片总结：</div><div><span>    1、每次执行作业都会判断是否需要分片：</span>作业信息变更需要重新分片<span>需要分片，</span>作业服务实例数量变更需要重新分片，没有变更则不需要分片。需要重新分片<span>会在zookeeper中创建结点</span>leader/sharding/necessary<br/></div><div><span>    2、分片作业借助zookeeper进行，每次分片前先判断有没有leader结点，没有则选举，有则在leader结点上计算分片。分片前先创建</span>leader/sharding/processing临时节点，分片后删除；分片信息会记录在sharding/instance中</div></span>
</div></body></html> 